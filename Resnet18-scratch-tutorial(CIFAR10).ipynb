{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet 18 구현\n",
    "\n",
    "출처 : [ResNet18_CIFAR10_Train.ipynb](https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/ResNet18_CIFAR10_Train.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic block 만들기\n",
    "- `nn.Conv2d` : 2d convolution layer (input_channel, output_channel, stride, padding, bias)  \n",
    "- `nn.BatchNorm2d` : Input이 2d 이미지일 때, 사용하는 Batch Normalization 기법  \n",
    "    Batch Normalization을 사용하는 이유? - layer를 통과해도 이미지 분포에 대한 변화를 처음과 비슷한 분포로 만들기 위해서\n",
    "    \n",
    "- `nn.Identity()` : Input과 동일한 값을 ouput으로 보내주기 위해 쓰는 함수\n",
    "- `nn.Sequential()` : 여러 layer를 쌓을 때 사용하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride = 1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # 3x3 필터를 사용 - 너비와 높이를 줄일 때는 stride 값을 조절\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes) # 배치 정규화(batch normalization)\n",
    "        \n",
    "        # 3x3 필터 사용 - 이번에는 너비와 높이가 동일\n",
    "        self.conv2 = nn.Conv2d(planes,planes,kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Identity() # identity인 경우\n",
    "        # 차원을 낮춰줘야 하는 경우\n",
    "        if stride != 1:            \n",
    "            self.shortcut = nn.Sequential(\n",
    "                            nn.Conv2d(in_planes, planes, kernel_size = 1, stride = stride, bias = False),\n",
    "                            nn.BatchNorm2d(planes)\n",
    "                            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  # resnet 논문의 핵심!\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Block을 여러번 묶어서 Resnet 구현\n",
    "- `_make_layer` : layer를 block의 수만큼 쌓아 올리는 함수\n",
    "    - strides = [ stride, 1 , 1, 1, ... ] - (1의 갯수가 num_blocks - 1만큼)\n",
    "    - layer는 가변인자를 ouput으로 내보내는 것  \n",
    "        참고 : [파이썬의 Asterisk(*) 이해하기](https://mingrammer.com/understanding-the-asterisk-of-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        \n",
    "        # 64개의 3x3 필터를 사용\n",
    "        self.conv1 = nn.Conv2d(3,64,kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride = 1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride = 2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride = 2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1) # out.size(0) = out.reshape(0)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet(CIFAR10) 다운로드 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root= './data', train = True, download = True, transform = train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root= './data', train = False, download = True, transform = test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 128, shuffle = True, num_workers = 4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet 18 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "cudnn.benchmark = True\n",
    "\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet18_cifar10.pth'\n",
    "\n",
    "# Loss function 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer 정의\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = 0.9, weight_decay = 0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d ]' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in  enumerate(train_loader):\n",
    "        inputs= inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()  # 가중치값 초기화 과정\n",
    "        loss = criterion(outputs, targets) # Loss 계산\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step() # 가중치 Update\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "            print('Current benign train loss:', loss.item())\n",
    "\n",
    "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
    "    print('Total benign train loss:', train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        total += targets.size(0)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print('\\nTest accuarcy:', 100. * correct / total)\n",
    "    print('Test average loss:', loss / total)\n",
    "\n",
    "    state = {\n",
    "        'net': net.state_dict()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning_rate 조절 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Train epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.125\n",
      "Current benign train loss: 2.377624273300171\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.1640625\n",
      "Current benign train loss: 2.0662007331848145\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.34375\n",
      "Current benign train loss: 1.8738585710525513\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.296875\n",
      "Current benign train loss: 1.9341106414794922\n",
      "\n",
      "Total benign train accuarcy: 25.782\n",
      "Total benign train loss: 844.2018893957138\n",
      "\n",
      "[ Test epoch: 0 ]\n",
      "\n",
      "Test accuarcy: 32.19\n",
      "Test average loss: 0.018502931606769562\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 1 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.25\n",
      "Current benign train loss: 1.8070673942565918\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.375\n",
      "Current benign train loss: 1.6769075393676758\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.3359375\n",
      "Current benign train loss: 1.6720048189163208\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.4375\n",
      "Current benign train loss: 1.6363450288772583\n",
      "\n",
      "Total benign train accuarcy: 40.228\n",
      "Total benign train loss: 630.6884019374847\n",
      "\n",
      "[ Test epoch: 1 ]\n",
      "\n",
      "Test accuarcy: 40.85\n",
      "Test average loss: 0.01650760700702667\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 2 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.515625\n",
      "Current benign train loss: 1.4258689880371094\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.40625\n",
      "Current benign train loss: 1.5566613674163818\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.3984375\n",
      "Current benign train loss: 1.5857077836990356\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.5\n",
      "Current benign train loss: 1.3403472900390625\n",
      "\n",
      "Total benign train accuarcy: 47.648\n",
      "Total benign train loss: 560.3070122003555\n",
      "\n",
      "[ Test epoch: 2 ]\n",
      "\n",
      "Test accuarcy: 49.36\n",
      "Test average loss: 0.013800116324424743\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 3 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.5234375\n",
      "Current benign train loss: 1.3527847528457642\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.5234375\n",
      "Current benign train loss: 1.3004150390625\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.5390625\n",
      "Current benign train loss: 1.187770962715149\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.625\n",
      "Current benign train loss: 1.0486478805541992\n",
      "\n",
      "Total benign train accuarcy: 54.112\n",
      "Total benign train loss: 493.68765914440155\n",
      "\n",
      "[ Test epoch: 3 ]\n",
      "\n",
      "Test accuarcy: 54.76\n",
      "Test average loss: 0.01278544670343399\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 4 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.53125\n",
      "Current benign train loss: 1.2845349311828613\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.625\n",
      "Current benign train loss: 1.1214960813522339\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6640625\n",
      "Current benign train loss: 1.0077935457229614\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.640625\n",
      "Current benign train loss: 1.0380916595458984\n",
      "\n",
      "Total benign train accuarcy: 62.428\n",
      "Total benign train loss: 415.1421290040016\n",
      "\n",
      "[ Test epoch: 4 ]\n",
      "\n",
      "Test accuarcy: 61.26\n",
      "Test average loss: 0.011511361485719681\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 5 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.8877388834953308\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.6875\n",
      "Current benign train loss: 0.8724702596664429\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.65625\n",
      "Current benign train loss: 0.9366232752799988\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6640625\n",
      "Current benign train loss: 0.9133833646774292\n",
      "\n",
      "Total benign train accuarcy: 67.328\n",
      "Total benign train loss: 360.24759685993195\n",
      "\n",
      "[ Test epoch: 5 ]\n",
      "\n",
      "Test accuarcy: 62.68\n",
      "Test average loss: 0.010563347387313843\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 6 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 0.7344789505004883\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.6248367428779602\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.734375\n",
      "Current benign train loss: 0.8502359986305237\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6328125\n",
      "Current benign train loss: 0.8729607462882996\n",
      "\n",
      "Total benign train accuarcy: 71.21\n",
      "Total benign train loss: 318.559446811676\n",
      "\n",
      "[ Test epoch: 6 ]\n",
      "\n",
      "Test accuarcy: 69.99\n",
      "Test average loss: 0.008802649158239365\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 7 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7578125\n",
      "Current benign train loss: 0.6367741823196411\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 0.7728858590126038\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 0.7408321499824524\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.703125\n",
      "Current benign train loss: 0.7406787276268005\n",
      "\n",
      "Total benign train accuarcy: 74.25\n",
      "Total benign train loss: 285.4805337190628\n",
      "\n",
      "[ Test epoch: 7 ]\n",
      "\n",
      "Test accuarcy: 71.95\n",
      "Test average loss: 0.008105361372232438\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 8 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.5664851069450378\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.734375\n",
      "Current benign train loss: 0.7420860528945923\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 0.6229400634765625\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 0.6796011328697205\n",
      "\n",
      "Total benign train accuarcy: 77.158\n",
      "Total benign train loss: 256.02514547109604\n",
      "\n",
      "[ Test epoch: 8 ]\n",
      "\n",
      "Test accuarcy: 74.71\n",
      "Test average loss: 0.007459850564599037\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 9 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.5344830751419067\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 0.5559717416763306\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.579653799533844\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.828125\n",
      "Current benign train loss: 0.5285158753395081\n",
      "\n",
      "Total benign train accuarcy: 79.456\n",
      "Total benign train loss: 229.42986416816711\n",
      "\n",
      "[ Test epoch: 9 ]\n",
      "\n",
      "Test accuarcy: 77.64\n",
      "Test average loss: 0.0065915043592453\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 10 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.49624282121658325\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.828125\n",
      "Current benign train loss: 0.47812676429748535\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.4096345901489258\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 0.5143705606460571\n",
      "\n",
      "Total benign train accuarcy: 81.22\n",
      "Total benign train loss: 210.18142783641815\n",
      "\n",
      "[ Test epoch: 10 ]\n",
      "\n",
      "Test accuarcy: 78.01\n",
      "Test average loss: 0.0064036750912666324\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 11 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.828125\n",
      "Current benign train loss: 0.5042767524719238\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.4966495633125305\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5700006484985352\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.4853508472442627\n",
      "\n",
      "Total benign train accuarcy: 82.628\n",
      "Total benign train loss: 195.3055353462696\n",
      "\n",
      "[ Test epoch: 11 ]\n",
      "\n",
      "Test accuarcy: 78.49\n",
      "Test average loss: 0.006387187817692756\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 12 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 0.4852686822414398\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.7734375\n",
      "Current benign train loss: 0.6149907112121582\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4114243984222412\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5678957104682922\n",
      "\n",
      "Total benign train accuarcy: 83.97\n",
      "Total benign train loss: 180.2014605998993\n",
      "\n",
      "[ Test epoch: 12 ]\n",
      "\n",
      "Test accuarcy: 81.28\n",
      "Test average loss: 0.005375569701194763\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 13 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.5238126516342163\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8828125\n",
      "Current benign train loss: 0.38506898283958435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 0.6133056282997131\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.48437556624412537\n",
      "\n",
      "Total benign train accuarcy: 84.706\n",
      "Total benign train loss: 171.62866127490997\n",
      "\n",
      "[ Test epoch: 13 ]\n",
      "\n",
      "Test accuarcy: 77.34\n",
      "Test average loss: 0.006857270067930222\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 14 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.4085725247859955\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.39944109320640564\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.828125\n",
      "Current benign train loss: 0.5140188336372375\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.41965219378471375\n",
      "\n",
      "Total benign train accuarcy: 85.614\n",
      "Total benign train loss: 162.85174638032913\n",
      "\n",
      "[ Test epoch: 14 ]\n",
      "\n",
      "Test accuarcy: 75.36\n",
      "Test average loss: 0.007408083790540696\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 15 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.2962079346179962\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.43729299306869507\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.828125\n",
      "Current benign train loss: 0.4129493236541748\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.4716477394104004\n",
      "\n",
      "Total benign train accuarcy: 86.396\n",
      "Total benign train loss: 154.01022671163082\n",
      "\n",
      "[ Test epoch: 15 ]\n",
      "\n",
      "Test accuarcy: 82.66\n",
      "Test average loss: 0.005187798334658146\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 16 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.35401177406311035\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.42885881662368774\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.4439992606639862\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.3517468571662903\n",
      "\n",
      "Total benign train accuarcy: 86.966\n",
      "Total benign train loss: 147.64216987788677\n",
      "\n",
      "[ Test epoch: 16 ]\n",
      "\n",
      "Test accuarcy: 82.51\n",
      "Test average loss: 0.005482040160894394\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 17 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.3420383334159851\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.3454287052154541\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 0.4788029193878174\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.48739075660705566\n",
      "\n",
      "Total benign train accuarcy: 87.286\n",
      "Total benign train loss: 141.95734523236752\n",
      "\n",
      "[ Test epoch: 17 ]\n",
      "\n",
      "Test accuarcy: 81.36\n",
      "Test average loss: 0.005858796623349189\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 18 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.921875\n",
      "Current benign train loss: 0.23689523339271545\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.3688437044620514\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.31821176409721375\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.2589770257472992\n",
      "\n",
      "Total benign train accuarcy: 87.624\n",
      "Total benign train loss: 138.66841506958008\n",
      "\n",
      "[ Test epoch: 18 ]\n",
      "\n",
      "Test accuarcy: 83.15\n",
      "Test average loss: 0.004990146464109421\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 19 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.953125\n",
      "Current benign train loss: 0.2167350947856903\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8828125\n",
      "Current benign train loss: 0.31586453318595886\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.3806647062301636\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.4722219407558441\n",
      "\n",
      "Total benign train accuarcy: 88.31\n",
      "Total benign train loss: 130.22632628679276\n",
      "\n",
      "[ Test epoch: 19 ]\n",
      "\n",
      "Test accuarcy: 80.96\n",
      "Test average loss: 0.005821545860171318\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 20):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
